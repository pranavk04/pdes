\chapter{Review and First Order PDEs}

\section{Introduction}
Contrary to ordinary differential equations, or ODEs, the \vocab{partial differential equation}, or PDE, is an equation of a multivariable unknown function. The set of solutions forms a set of surfaces, unlike the set of curves that is formed from the solutions to ordinary differential equations. 

Just like ordinary differential equations, partial differential equations have particular solutions. 
\begin{definition}{}{}
An \vocab{initial condition} is a condition, such as $u(x,0)$, where the value of the variable is set at a specific value. Contrasting this is the \vocab{boundary condition} where we specify the value of the solution for a fixed spatial variable, such as $u(0,t) = \cos{t}$. 
\end{definition}

\section{Vector Calculus}
Recall the vector field:
\begin{definition}{}{}
A \vocab{vector field} $\mathbf{F}$ in $\RR^2$ takes a point $(x,y)$ and puts it to a vector $\mathbf{F}(x,y)$ in $\RR^2$.
\end{definition}

We can think of the vector field as parameterized by two scalar functions, each acting on $x$ and $y$. There are two familiar functions of the vector field: 
\begin{itemize}
\item The \vocab{gradient} of a function $\mathbf{F}$ at a point is 
\[ \nabla f = \begin{pmatrix} f_x \\ f_y \end{pmatrix}. \] 
We can quickly generate a vector field from a function by the gradient. Said vector field is said to be \vocab{conservative}: $\mathbf{F} = \nabla f$ for some function $f$. 
\item The \vocab{divergence} of a vector field $\nabla \cdot \mathbf{F}$, where 
\[ \mathbf{F} = \begin{pmatrix} F_1 \\ F_2 \end{pmatrix} \] is 
\[ \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y}. \] 
\end{itemize}

Combining these opeations yields a \emph{scalar} function: 
\begin{definition}{Laplacian}{}
The \vocab{laplacian} of a function $f$ is equivalent to $\nabla^2 f$, or the divergence of the gradient of $f$. We can express it as 
\[ \nabla^2 f = f_{xx} + f_{yy}. \] We also have the notation $\Delta f$. 

\end{definition}

If $\mathbf{F} \in \RR^3$, then an additional operation, the \vocab{curl} exists, and is defined as 
\[ \curl{\mathbf{F}} = \nabla \times \mathbf{F} = \left | \begin{bmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ F_1 & F_2 & F_3 \end{bmatrix} \right | . \] 

In higher dimensions, recall that the surface, line, and volume integrals exist in place of the standard integral. We may integrate over both scalar functions and vector fields, and the surface integral of a vector field is known as a \vocab{flux integral}. 

We also have the following important theorems:
\begin{theorem}{The Divergence Theorem}{}
Let $D \subset \RR^2$ be a smooth domain with a positively oriented boundary $\partial D$ and let $\mathbf{F}$ be a smooth vector field. Then 
\[ \iint_D \nabla \cdot D \diff A = \int_{\partial D} \mathbf{F} \cdot \mathbf{n} \diff s. \]
\end{theorem}
This theorem is also known as \vocab{Gauss' Theorem}. In three dimensions, the left and right integrals are a volume and surface integral, respectively. 

\begin{theorem}{Stoke's Theorem}{}
Let $\mathbf{F}$ be a smooth vector field and $\mathcal S$ be a smooth surface in $\RR^3$ with bounding curve $C$. Then 
\[ \iint_\mathcal{S} (\nabla \times \mathbf{F}) \cdot \mathbf{n} \diff \mathcal{S} = \int_C \mathbf{F} \cdot \mathbf{t} \diff s. \] 
\end{theorem}

The surface integral over $\mathcal{S}$ is equivalent to the line integral over the bounding curve $C$. 


\section{Ordinary Differential Equations}
Linearity is an important property for differential equations, and we can phrase things in terms of \vocab{linear operators}. Recall that the derivative, the integral, and matrix multiplication (on a vector) are all linear operators. We can now define, formally, what a \vocab{linear differential equation} is. 

\begin{definition}{}{}
A differential equation is linear if it can be written in terms of operators, such that 
\[ \mathcal{L}u = f, \] where $\mathcal L$ is a linear differential operator, and $f$ does not depend on $u$. If $f \equiv 0$, then our equation is \vocab{homogeneous}. Otherwise, it is \vocab{nonhomogeneous}. 
\end{definition}

While linearity at first may not seem powerful, it leads to important properties such as the following: 
\begin{theorem}{Superposition Principle}{}
If $\{u_1, u_2, \cdots, u_n \}$ all satisfy the linear equation $\mathcal{L}u = 0$, then any linear combination 
\[ \sum_{k=0}^n c_ku_k \] also satisfies $\mathcal{L}u = 0$. 
\end{theorem}
\begin{proof}
Since $\mathcal{L}$ is a linear operator, we have 
\[ \CL \left ( \sum_{k=0}^n c_ku_k \right ) = \sum_{k=0}^n \CL(c_ku_k) = \sum_{k=0}^n c_k\CL u_k = c_1 \cdot 0 + c_2 \cdot 0 + \cdots + c_n \cdot 0 = 0, \] and the result follows. 
\end{proof}

With this, we can describe common forms of linear equations.

The first order linear equation has two types: homogeneous and nonhomogeneous. In the homogeneous case, the equation reduces to a separable differential equation, which can be solved by a direct integration. However, consider the nonhomogeneous case. We have 
\[ y'(t) + p(t)y(t) = f(t). \] Define the \vocab{integrating factor} 
\[ m(t) = e^{\int p(t) \diff t}. \] Multiplying both sides of the equation by $m(t)$ will reduce to 
\[ m(t)y(t) = \int m(t)f(t) \diff t + C \Longleftrightarrow y(t) = \frac{\int m(t)f(t) \diff t + C}{m(t)}. \] This is the general solution form of the nonhomogeneous linear equation. 

The second order linear equation, is of the form 
\[ ay''(t) + by'(t) + cy(t) = 0, \] where $a,b,c \in \RR$ are constant coefficients. Solutions will be of the form $y(t) = e^t$, and from this we can derive the \vocab{characteristic equation}, the polynomial 
\[ ar^2 + br + c = 0. \] The solutions of the characteristic equation determine the solutions of the equation itself: 
\begin{itemize}
\item Pairs of distinct real roots: The equation thus has two linearly independent solutions $e^{r_1t}$ and $e^{r_2t}$, and by superposition our general solution is 
\[ y(t) = c_1e^{r_1t} + c_2e^{r_2 t}. \] 
\item One repeated real root: We have one solution, and can force linear independence by multiplying by a factor of $t$, thus the general solution is of the form 
\[ y(t) = c_1e^{r_1 t} + c_2te^{r_2 t}. \] 
\item Pairs of complex roots: We can reduce these to sines and cosines due to the complex nature of the roots. If $r_i = \alpha \pm \beta i$, then the general solution is of the form 
\[ y(t) = c_1e^{\alpha t}\cos{\beta t} + c_2e^{\alpha t}\sin{\beta t}. \] 
\end{itemize}

Solving for a \vocab{particular solution} can be done provided that \vocab{initial conditions} are given for the problem, in which it is a simple linear system of equations to determine the values of $c_1$ and  $c_2$. 

A \vocab{boundary value} problem is when the initial conditions are at different values of $t$, such as $y(0) = 4$ and $y(4) = \pi$. The location and values at the boundary can drastically change what sorts of particular solutions are obtained, but solving for these is once again a linear system of equations. 

However, we must consider the case that the coefficients are nonconstant, that is, the equation is of the form 
\[ a(x)y''(x) + b(x)y'(x) + c(x)y(x) = 0. \] Unfortunately, these are rather complex, and instead we limit our focus to equations known as \vocab{Cauchy-Euler equations}, which satisfy the form 
\[ ax^2y''(x) + bxy'(x) + cy(x) = 0, \] where $a,b,c \in \RR$ and $a \neq 0$. 

To approach these, we can once again guess the form of the solution, but this time, we use $y(x) = x^p$, such that the \vocab{Cauchy-Euler characteristic equation} is of the form 
\[ ap^2 + (b-1)p + c = 0. \] We can once again consider the cases of the roots to determine the general solution: 
\begin{itemize}
\item Pairs of distinct real roots: Similar to the previous instance with constant coefficients, the general solution is of the form 
\[ y(x) = x_1x^{p_1} + c_2x^{p_2}. \] 
\item One repeated real root: Instead of multiplying by $x$, as this would change the degree (and thus, the implied root) of the solution, we instead multiply by $\log{x}$, resulting in a general solution of 
\[ y(x) = c_1x^{p_1} + c_2x^{p_1}\log{x}. \] 
\item In the case of complex roots, we have a similar form, except this time the arguments of sine and cosine are multiplied by $\log{x}$, resulting in a general solution of 
\[ y(x) = c_1x^\alpha\cos{\beta \log{x}} + c_2x^\alpha \sin{\beta \log{x}}. \] 
\end{itemize}

Often times when working with boundary value problems, it is easier to write the general solutions in terms of hyperbolic trigonometric functions instead of $e^x$. Supposing we have a solution of the form 
\[ y(x) = c_1e^{r_1 x} + c_2e^{-r_1 x}, \] then we can rewrite this in terms of hyperbolic sine and cosine to obtain 
\[ y(x)  = c_3\cosh{r_1x} + c_4\sinh{r_1x}. \] This is best used when the characteristic equation is of the form 
\[ r^2 - n = 0. \] 


\section{First Order PDEs}

We can motivate the study of partial differential equations by considering the familiar class of theorems known as \vocab{conservation laws}. Denote the density of some quantity as $u(x,t)$, and any variation of a state variable should be restricted to one dimension. For instance, imagine a cylindrical wire. $u$ should be the same across any point in a cross section of the wire.

Let $\phi(x,t)$ denote the \vocab{flux} of $x$ at a time $t$. Then the amount of the quantity crossing the section at $x$ with time $t$ is equivalent to $A\phi(x,t)$ where $A$ is the cross sectional area of the section. If we let $f(x,t)$ be the rate at which the quantity enters the system (or is destroyed), called a \vocab{source} or \vocab{sink}, we have $f(x,t)A \diff x$ as the amount of quantity created in a small width of $\diff x$ at some time $t$. 

Formulating this mathematically, for some interval $[a,b]$, we can write 
\[ \frac{\diff}{\diff t} \int_a^b u(x,t)A \diff x = A\phi(a,t) - A\phi(b,t) + \int_a^b f(x,t)A\diff x. \] 

Assuming sufficient smoothness of $f$ and $u$, we can rewrite this as 
\[ \int_a^b u_t(x,t) + \phi_x(x,t) - f(x,t) \diff x = 0. \] Since $[a,b]$ is arbitrary, we can write this as a fundamental differential equation: 
\begin{definition}{Fundamental Conservation Law}{}
\[ u_t(x,t) + \phi_x(x,t) = f(x,t). \] 
\end{definition}

Many equations of this form are motivated by physical phenomena. Sometimes, $\phi$ and $f$ may depend explicitly on $u$, so that $f = f(u)$, for example. These are called \vocab{constitutive relations}. 

We can now learn how to solve basic first order PDEs by the \vocab{method of characteristics}. 
Consider a model where the flux is proportional to the density itself, or $\phi = cu$. If $c$ is a constant, this is called the \vocab{advection model}. 
\begin{definition}{Advection Equation}{168G}
From the above, we can write a conservation law of 
\[ u_t + cu_x = 0. \] 
\end{definition}

We can see that a function $u(x,t) = F(x - ct)$ is a solution to this equation (which is evident by the chain rule). This is the \vocab{general solution} of the advection equation, and it has the form of a right travelling wave. Let's consider a general form of this problem.
\begin{example}{Advection Initial Value}{}
Suppose we have 
\[ u_t + cu_x = 0, \] where $x \in \RR$, $t$ is positive, and $u(x,0) = u_0(x)$. Evidently, we have $u(x,t) = u_0(x - ct)$ as a solution. 
\end{example}
Physically, we can interpret this as the wave moving to the right at a speed of $c$. We can also think of this as the wave moving along the family of parallel straight lines $\xi = x - ct$, constant in space-time. These lines are called \vocab{characteristics}. 

The solution behaves in a way such that the strength of the density, $u$, remains constant along any characteristic curve. 

Let's consider a general version of this problem. 
\begin{example}{General Advection Equation}{}
Consider the general advection equation, which has the form 
\[ u_t + cu_x + au = f(x,t). \] 
Since the equation is propagating waves at speed $c$, we can transform it into a moving coordinate system. These are caled \vocab{characteristic coordinates}, defined as 
\[ \xi = x - ct, \tau = t. \] We can interpret $\xi$ as a moving coordinate that travels with the wave. We can write $u(x,t)$ as $U(\xi, \tau) = u(\xi+c\tau, \tau)$. From the chain rule, we have 
\[ u_t = U_\xi\xi_t + U_\tau\tau_t = -cU_\xi + U_\tau, \] and we have that 
\[ u_x = U_\xi\xi_x + U_\tau\tau_x = U_\xi. \] We can rewrite the general form of the advection equation as 
\[ U_\tau + aU = F(\xi, \tau). \] Notice that this is simply an ordinary differential equation in $\tau$! We have reduced the partial differential equation to an ordinary one, which we can solve by normal linear methods. 
\end{example}

Let's try another problem, with specific coefficients. 
\begin{exercise}
Find the general solution to 
\[ u_t + 2u_x - u = t. \] 
\end{exercise}
\begin{sol}
Let $\xi = x - 2t$, and let $\tau = t$. Recall the characteristic coordinates transform the equation to 
\[ U_\tau - U = \tau. \] 	The integration factor is thus -1, and multiplying by $e^{-\tau}$ yields 
\[ \frac{\partial}{\partial \tau}Ue^{-\tau} = \tau e^{-\tau}. \] Solving this yields 
\[ Ue^{-\tau} = -(1 + \tau)e^{-\tau} + g(\xi), \] where $g$ is some arbitrary function. We can then transform back to the $x$ and $t$ variables to yield the general solution of 
\[ \boxed{u(x,t) = -(1+t) + g(x - 2t)e^t}. \] 
\end{sol}


